# -*- coding: utf-8 -*-
"""attnlstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JZhAoNI6vmGapwShLzlwLv-rYji4ifHX
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from keras.models import Sequential
from keras.layers import LSTM, Dense
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from keras.layers import Dropout

df = pd.read_csv('/content/ltedata.csv')

data = df
data.head(10)

train_data = data['Length'].astype(float)
train_data.info()

dfp=train_data.tail(100)
dfp.plot.line()

numeric_columns = data.select_dtypes(include=['float64']).columns
numeric_data = data[numeric_columns]

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform only the numeric data
scaled_data = scaler.fit_transform(numeric_data)

scaled_data

def split_sequence(sequence, n_steps):
	X, y = list(), list()
	for i in range(len(sequence)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequence)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
		X.append(seq_x)
		y.append(seq_y)
	return np.array(X), np.array(y)

X, y = split_sequence(scaled_data, 20)

X = X.reshape((X.shape[0], X.shape[1], 1))

import tensorflow as tf
from tensorflow.keras.layers import Layer
from tensorflow.keras import backend as K

class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], input_shape[-1]), initializer="glorot_uniform", trainable=True)
        self.b = self.add_weight(name="att_bias", shape=(input_shape[-1],), initializer="zeros", trainable=True)
        self.u = self.add_weight(name="att_u", shape=(input_shape[-1],), initializer="glorot_uniform", trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, x):
        # Compute the attention scores
        uit = K.tanh(K.dot(x, self.W) + self.b)
        ait = K.dot(uit, K.expand_dims(self.u))
        ait = K.squeeze(ait, -1)
        ait = K.exp(ait)
        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        # Apply attention scores to the input
        ait = K.expand_dims(ait)
        weighted_input = x * ait
        output = K.sum(weighted_input, axis=1)
        return output

    def compute_output_shape(self, input_shape):
        return input_shape[0], input_shape[-1]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, Input
from tensorflow.keras.models import Model

# Define the input layer
input_layer = Input(shape=(X.shape[1], X.shape[2]))

# Define the LSTM layers
lstm1 = LSTM(256, activation='relu', return_sequences=True)(input_layer)
lstm2 = LSTM(128, activation='relu', return_sequences=True)(lstm1)
lstm3 = LSTM(64, activation='relu', return_sequences=True)(lstm2)

# Add the attention layer
attention = Attention()(lstm3)

# Add dropout and dense layers
dropout = Dropout(0.1)(attention)
dense1 = Dense(10, activation='relu')(dropout)
output_layer = Dense(1)(dense1)

# Define the model
model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(loss='mse', optimizer='adam', metrics=['mae'])

# Summary of the model
model.summary()

hist = model.fit(X, y, epochs=1000,batch_size = 258, validation_split=0.01, verbose=1)

# Plot training loss
plt.plot(hist.history['loss'], label='Training Loss')

# Plot validation loss
plt.plot(hist.history['val_loss'], label='Validation Loss')

# Customize the plot
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.plot(hist.history['mae'], label='Training MAE')

# Plot validation MAE
plt.plot(hist.history['val_mae'], label='Validation MAE')

# Customize the plot
plt.title('Training and Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()
plt.show()

forecast= model.predict(X[-200:])

hist.history

unscaled_data = scaler.inverse_transform(forecast)

unscaled_data

kat = unscaled_data.flatten()

indic = data['date'][-200:]

df_fore = pd.DataFrame({'index':indic,'length':kat})

dfp=df_fore.tail(200)

sns.lineplot(dfp )

dp=train_data.tail(200)

dp.plot.line()
dfp['length'].plot.line()

